{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29ce239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Sample text data (replace this with loading your dataset)\n",
    "text_data = \"ðŸ˜Š This is an example text with emojis and HTTP links: https://example.com ðŸ˜Ž It also has some grammatical errors and various words in different forms. Lemmatization and stemming will help in normalizing the words. The stop words like 'the' and 'is' should be removed. Let's perform these preprocessing tasks for better text analysis!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d0f4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cbbf0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoji Removal\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                           u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                           u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                           u\"\\U000024C2-\\U0001F251\" \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "text_data = emoji_pattern.sub(r'', text_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "784bef16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kashi\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b69976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\kashi\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c4807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3afd527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text_data = ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text_data)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72bd5aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "text_data = ' '.join([stemmer.stem(word) for word in word_tokenize(text_data)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f74de1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Tokenization\n",
    "word_tokens = word_tokenize(text_data)\n",
    "\n",
    "# Grammar Correction (using TextBlob)\n",
    "textblob_object = TextBlob(text_data)\n",
    "text_data = str(textblob_object.correct())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aadb066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP Links Removal\n",
    "text_data = re.sub(r'http\\S+', '', text_data)\n",
    "\n",
    "# Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b25413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e707f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the is an example text with emboli and http link  http  examplecom it also ha some grammar error and various word in differ form  emma and stem will help in normal the word  the stop word like the  and i  should be remove  let s perform these preprocess task for better text analysis \n"
     ]
    }
   ],
   "source": [
    "print(text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3887e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the is an example text with emboli and http link  http  examplecom it also ha some grammar error and various word in differ form  emma and stem will help in normal the word  the stop word like the  and i  should be remove  let s perform these preprocess task for better text analysis \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Sample text data (replace this with loading your dataset)\n",
    "text_data = \"ðŸ˜Š This is an example text with emojis and HTTP links: https://example.com ðŸ˜Ž It also has some grammatical errors and various words in different forms. Lemmatization and stemming will help in normalizing the words. The stop words like 'the' and 'is' should be removed. Let's perform these preprocessing tasks for better text analysis!\"\n",
    "\n",
    "# Emoji Removal\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                           u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                           u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                           u\"\\U000024C2-\\U0001F251\" \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "text_data = emoji_pattern.sub(r'', text_data)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text_data = ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text_data)])\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "text_data = ' '.join([stemmer.stem(word) for word in word_tokenize(text_data)])\n",
    "\n",
    "# Word Tokenization\n",
    "word_tokens = word_tokenize(text_data)\n",
    "\n",
    "# Grammar Correction (using TextBlob)\n",
    "textblob_object = TextBlob(text_data)\n",
    "text_data = str(textblob_object.correct())\n",
    "\n",
    "# HTTP Links Removal\n",
    "text_data = re.sub(r'http\\S+', '', text_data)\n",
    "\n",
    "# Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentence_tokens = sent_tokenize(text_data)\n",
    "\n",
    "# Lower casing\n",
    "text_data = text_data.lower()\n",
    "\n",
    "# Remove white spaces\n",
    "text_data = ' '.join(text_data.split())\n",
    "\n",
    "# Text Normalization\n",
    "# Convert contractions to their expanded forms\n",
    "contractions = {\"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\", \"'ll\": \" will\", \"'ve\": \" have\", \"'re\": \" are\", \"'m\": \" am\"}\n",
    "for contraction, expansion in contractions.items():\n",
    "    text_data = text_data.replace(contraction, expansion)\n",
    "\n",
    "# Remove punctuation\n",
    "text_data = text_data.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "\n",
    "# Part of Speech Tagging\n",
    "pos_tags = nltk.pos_tag(word_tokens)\n",
    "\n",
    "# Now, 'text_data' contains the preprocessed and normalized text\n",
    "print(text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135fd235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
